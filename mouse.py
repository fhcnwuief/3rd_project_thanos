# mouseì—°ê²° ver0.8_4

import cv2
import mediapipe as mp
import numpy as np
import pyautogui
import subprocess       # virtual keyboard ë‚˜íƒ€ë‚¼ ë•Œ í•„ìš” 
import os               # virtual keyboard ë‹«ì„ ë•Œ í•„ìš”         
import pygetwindow as gw
import psutil
import time

pyautogui.FAILSAFE = False

# ë‹¤ë¥¸ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ Controller.flagë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
class Controller:
    flag = False

max_num_hands = 1
gesture = {
    0:'fist', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five',
    6:'six', 7:'rock', 8:'spiderman', 9:'yeah', 10:'ok', 11:'fy'
}

# MediaPipe hands model
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    max_num_hands=max_num_hands,
    min_detection_confidence=0.8,
    min_tracking_confidence=0.8)

# Define the screen resolution (you may adjust this based on your monitor)
screen_width, screen_height = pyautogui.size()   # ì¶”ê°€
print(screen_width, screen_height)

# Gesture recognition model
file = np.genfromtxt('./data/gesture_train_fy.csv', delimiter=',')
# file = np.genfromtxt('./BB_son/data/gesture_train_fy.csv', delimiter=',')
angle = file[:,:-1].astype(np.float32)
label = file[:, -1].astype(np.float32)
knn = cv2.ml.KNearest_create()
knn.train(angle, cv2.ml.ROW_SAMPLE, label)

cap = cv2.VideoCapture(0)
next_cnt = 0        # ìˆ˜ì • ë³€ìˆ˜ì´ˆê¸°í™”  ë™ì¼í•œ gesture count


# ì‹¤í–‰í•  vkeyboard.pyw íŒŒì¼ ê²½ë¡œ
pyw_file_path = './vkeyboard.pyw'       # ê²½ë¡œìˆ˜ì • í•´ì£¼ì„¸ìš”
# pyw_file_path = './vkey_main/vkeyboard.pyw'       # ê²½ë¡œìˆ˜ì • í•´ì£¼ì„¸ìš”
try:
    # .pyw íŒŒì¼ ì‹¤í–‰
    subprocess.Popen(['pythonw', pyw_file_path])
except FileNotFoundError:
    print("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")   # ì˜ˆì™¸ì²˜ë¦¬
except Exception as e:
    print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")


while cap.isOpened():
    ret, img = cap.read()
    
    if not ret:
        continue
 
    img = cv2.flip(img, 1)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    result = hands.process(img)

    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    image_height, image_width, _ = img.shape

    # ì¹´ë©”ë¼ í™”ë©´ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    camera_x = int(cap.get(3))
    camera_y = int(cap.get(4))
    
    # ì‚¬ê°í˜• ê·¸ë¦¬ê¸° (ì˜ˆ: í™”ë©´ ì¤‘ì•™ì— ì‚¬ê°í˜• ê·¸ë¦¬ê¸°)
    # rect_width = 480
    # rect_height = 360
    rect_width = 1048    # macbook
    rect_height = 480    # macbook
    rect_x = (camera_x - rect_width) // 2
    rect_y = (camera_y - rect_height) // 2
    rect_right = rect_x + rect_width
    rect_bottom = rect_y + rect_height
    
    cv2.rectangle(img, (rect_x, rect_y), (rect_x + rect_width, rect_y + rect_height), (255, 255, 0), 2)

    # í”„ë ˆìž„ í‘œì‹œ
    #cv2.imshow("Camera Feed", img)  


    if result.multi_hand_landmarks is not None:
        for res in result.multi_hand_landmarks:
            joint = np.zeros((21, 3))
            for j, lm in enumerate(res.landmark):
                joint[j] = [lm.x, lm.y, lm.z]

            # Compute angles between joints
            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:] # Parent joint
            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:] # Child joint
            v = v2 - v1 # [20,3]
            # Normalize v
            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

            # Get angle using arcos of dot product
            angle = np.arccos(np.einsum('nt,nt->n',
                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], 
                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]

            angle = np.degrees(angle) # Convert radian to degree

            # Inference gesture
            data = np.array([angle], dtype=np.float32)
            ret, results, neighbours, dist = knn.findNearest(data, 3)
            idx = int(results[0][0])

            # fyë™ìž‘ì¼ë•Œ ëª¨ìžì´í¬ ì²˜ë¦¬ë˜ë©´ì„œ escê¸°ëŠ¥
            if idx == 11:
                x1, y1 = tuple((joint.min(axis=0)[:2] * [img.shape[1], img.shape[0]] * 0.95).astype(int))
                x2, y2 = tuple((joint.max(axis=0)[:2] * [img.shape[1], img.shape[0]] * 1.05).astype(int))

                fy_img = img[y1:y2, x1:x2].copy()
                fy_img = cv2.resize(fy_img, dsize=None, fx=0.05, fy=0.05, interpolation=cv2.INTER_NEAREST)
                fy_img = cv2.resize(fy_img, dsize=(x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)

                img[y1:y2, x1:x2] = fy_img
                
                
                pyautogui.press('esc')
                

            
            # rockì¼ë•Œ ë§ˆìš°ìŠ¤ ë“œëž˜ê·¸
            if idx == 0:
                #print('mouse drag')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
               
                # ì†ëª©ì¢Œí‘œ ìž…ë ¥
                wrist_x, wrist_y = tuple((res.landmark[mp_hands.HandLandmark.WRIST].x * image_width, res.landmark[mp_hands.HandLandmark.WRIST].y * image_height))
                # Map the fingertip coordinates to the screen resolution
                # ì†ëª©ì¢Œí‘œë¥¼ ì‚¬ê°í˜• ë‚´ë¶€ì˜ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜
                relative_x = wrist_x - rect_x
                relative_y = wrist_y - rect_y
                # ìƒëŒ€ì¢Œí‘œë¥¼ ì „ì²´í™”ë©´ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ì¡°ì •
                screen_x = int(relative_x * screen_width / rect_width)
                screen_y = int(relative_y * screen_height / rect_height)
                # ë§ˆìš°ìŠ¤ ì¢Œí‘œ ì„¤ì •
                position_x = rect_x + screen_x
                position_y = rect_y + screen_y
                
           
                if next_cnt > 2:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0
                    if not Controller.flag:
                        Controller.flag = True
                        pyautogui.mouseDown(button='left', x=position_x, y=position_y)
                    else:
                        pyautogui.moveTo(position_x, position_y, duration=0.1)  # Adjust the values as needed
                   
                else:
                    continue

            # paperì¼ë•Œ ë§ˆìš°ìŠ¤ leftë“œë¡­
            if idx == 5:
                #print('mouse left click')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                
                # ì†ëª©ì¢Œí‘œ ìž…ë ¥
                wrist_x, wrist_y = tuple((res.landmark[mp_hands.HandLandmark.WRIST].x * image_width, res.landmark[mp_hands.HandLandmark.WRIST].y * image_height))
                # Map the fingertip coordinates to the screen resolution
                # ì†ëª©ì¢Œí‘œë¥¼ ì‚¬ê°í˜• ë‚´ë¶€ì˜ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜
                relative_x = wrist_x - rect_x
                relative_y = wrist_y - rect_y
                # ìƒëŒ€ì¢Œí‘œë¥¼ ì „ì²´í™”ë©´ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ì¡°ì •
                screen_x = int(relative_x * screen_width / rect_width)
                screen_y = int(relative_y * screen_height / rect_height)
                # ë§ˆìš°ìŠ¤ ì¢Œí‘œ ì„¤ì •
                position_x = rect_x + screen_x
                position_y = rect_y + screen_y
            
                if next_cnt > 2:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0    
                    if Controller.flag:
                        Controller.flag = False
                        pyautogui.mouseUp(button='left')
                        pyautogui.moveTo(position_x, position_y, duration=0.1)
                    else :
                        pyautogui.moveTo(position_x, position_y, duration=0.1)
                else:
                    continue
                    

                
            # 8ì€ 'spiderman'ì— í•´ë‹¹í•˜ëŠ” ì œìŠ¤ì²˜ìž…ë‹ˆë‹¤.           
            if idx == 8:  
                #print('page refresh')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                if next_cnt > 4:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 3ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0
                    pyautogui.press('F5')
                else:
                    continue
                
            if idx == 10:  # 10ì€ 'okay'ì— í•´ë‹¹í•˜ëŠ” ì œìŠ¤ì²˜ìž…ë‹ˆë‹¤.
                #print('mouse doubleclick')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                if next_cnt > 4:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0 
                    pyautogui.doubleClick()
                else:
                    continue
                    
            if idx == 9:  # 9ì€ 'yeah=V'ì— í•´ë‹¹í•˜ëŠ” ì œìŠ¤ì²˜ìž…ë‹ˆë‹¤.
                #print('right click')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                if next_cnt > 4:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0 
                    pyautogui.rightClick()
                else:
                    continue
                    
                    
            if idx == 4:  # 4ì€ '4'ì— í•´ë‹¹í•˜ëŠ” ì œìŠ¤ì²˜ìž…ë‹ˆë‹¤.
                #print('wind+d')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                if next_cnt > 15:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0 
                    pyautogui.hotkey('win','d')
                else:
                    continue
                    
                    
            if idx == 6:  # 6ì€ ðŸ¤™ì— í•´ë‹¹í•˜ëŠ” ì œìŠ¤ì²˜ìž…ë‹ˆë‹¤.
                #print('backspace')
                next_cnt += 1
                #print('next_cnt: ', next_cnt)
                if next_cnt > 4:        #ë˜‘ê°™ì€ ë™ìž‘ì´ 5ë²ˆ ì¸ì‹ë˜ì–´ì•¼ ì‹¤í–‰
                    next_cnt = 0 
                    pyautogui.press('backspace')
                else:
                    continue
                 
    cv2.imshow('Filter', img)
    if cv2.waitKey(1) == 27:   # q->esc
        break


    
cap.release()
cv2.destroyAllWindows()
cv2.waitKey(1)
# click(botton='right')